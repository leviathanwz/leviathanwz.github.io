<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>RNNs - Tag - loveLZZ的个人博客</title>
        <link>https://leviathanion.github.io/tags/rnns/</link>
        <description>RNNs - Tag - loveLZZ的个人博客</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>chu.quntao@gmail.com (loveLzz)</managingEditor>
            <webMaster>chu.quntao@gmail.com (loveLzz)</webMaster><lastBuildDate>Sat, 19 Feb 2022 10:09:43 &#43;0800</lastBuildDate><atom:link href="https://leviathanion.github.io/tags/rnns/" rel="self" type="application/rss+xml" /><item>
    <title>QRNN</title>
    <link>https://leviathanion.github.io/qrnn/</link>
    <pubDate>Sat, 19 Feb 2022 10:09:43 &#43;0800</pubDate>
    <author>loveLZZ</author>
    <guid>https://leviathanion.github.io/qrnn/</guid>
    <description><![CDATA[<h1 id="qrnn1">QRNN<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></h1>
<h2 id="novelty">Novelty</h2>
<ul>
<li>将RNN与CNN结合
<ul>
<li>卷积层用一种不使用未来数据的masked卷积来代替LSTM中参数矩阵与上一时刻隐藏层相乘的操作。捕获过去时刻信息的同时简化了LSTM中的的操作，因为在计算过程中删除了隐藏层，仅仅使用输入数据来捕获依赖</li>
<li>池化层使用各种门结构，例如LSTM的门结构和GRU的门结构使得梯度流稳定</li>
</ul>
</li>
<li>通过上述操作简化了LSTM的计算，使其可以并行</li>
</ul>
<h2 id="问题和动机">问题和动机</h2>
<ul>
<li><strong>标准的RNN包括门变种LSTM</strong>等因为<strong>无法并行计算</strong>，因此在<strong>长序列</strong>的任务中性能受到了限制。</li>
<li>将<strong>CNN</strong>用于序列模型时
<ul>
<li><strong>并行性更好</strong></li>
<li>可以<strong>更好地扩展到长序列</strong></li>
<li>但因为<strong>最大和平均池化时假设了时间不变性</strong>，（在一次卷积池化过程中，时间步的顺序会被忽略，移动卷积核的过程中，进行相同的池化操作，不同时间步的重要性不同同样也会被忽略）因此无法充分利用<strong>大规模序列的顺序信息</strong>。</li>
</ul>
</li>
<li>因此作者提出了一种将<strong>CNN和RNN混合</strong>的模型QRNN，既能<strong>跨时间步和小批量维度进行并行计算</strong>，又使得<strong>输出取决于总体顺序</strong>。<strong>性能更优秀且更节省时间</strong></li>
</ul>
<h2 id="过去的解决方法">过去的解决方法</h2>
<ul>
<li>
<p>将CNN应用到序列模型<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>character-level CNN(NIPS2015)假设了时间不变性，无法利用顺序信息
<figure><a class="lightgallery" href="/QRNN/character-levelCNN.png" title="character-levelCNN模型" data-thumbnail="/QRNN/character-levelCNN.png" data-sub-html="<h2>该模型主要通过多个卷积层和池化层堆叠，沿时间步对序列进行卷积和池化来实现序列信息的捕获</h2><p>character-levelCNN模型</p>]]></description>
</item>
<item>
    <title>SCRN</title>
    <link>https://leviathanion.github.io/scrn/</link>
    <pubDate>Thu, 17 Feb 2022 15:42:05 &#43;0800</pubDate>
    <author>loveLZZ</author>
    <guid>https://leviathanion.github.io/scrn/</guid>
    <description><![CDATA[<h1 id="scrn模型1">SCRN模型<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></h1>
<h2 id="novelty">Novelty</h2>
<ul>
<li>通过为RNN添加一个参数缓慢变化的隐藏层来捕获长距离依赖，公式如下所示</li>
</ul>
<p>\begin{align*}
s_t &amp;= (1-\alpha)Bx_t + \alpha s_{t-1}
\\
h_t &amp;= \sigma(Ps_t+Ax_t+Rh_{t-1})
\\
y_t &amp;= f(Uh_t+Vs_t)
\end{align*}</p>
<ul>
<li>约束该隐藏层中的B为对角矩阵，由于B为对角矩阵，且没有使用激活函数，所以该隐藏层的梯度流稳定
<figure><a class="lightgallery" href="/SCRN/RNN%E5%92%8CSCRN%E6%9E%B6%E6%9E%84%E5%9B%BE.png" title="RNN和SCRN的架构图" data-thumbnail="/SCRN/RNN%E5%92%8CSCRN%E6%9E%B6%E6%9E%84%E5%9B%BE.png" data-sub-html="<h2>RNN和SCRN的架构图</h2><p>RNN和SCRN的架构图</p>]]></description>
</item>
<item>
    <title>IRNN</title>
    <link>https://leviathanion.github.io/irnn/</link>
    <pubDate>Thu, 23 Dec 2021 10:27:32 &#43;0800</pubDate>
    <author>loveLZZ</author>
    <guid>https://leviathanion.github.io/irnn/</guid>
    <description><![CDATA[<h1 id="irnn模型1">IRNN模型<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></h1>
<h2 id="novelty">Novelty</h2>
<ul>
<li>专注于解决RNN模型中的梯度消失问题</li>
<li>使用单位矩阵来初始化RNN从而部分解决梯度消失和梯度爆炸问题</li>
<li>使用ReLU激活函数代替Sigmoid激活函数也用于解决梯度消失问题（要配合第二点，否则很有可能梯度爆炸）</li>
</ul>
<h2 id="问题和动机">问题和动机</h2>
<ul>
<li><strong>梯度消失</strong>和<strong>梯度爆炸</strong>的问题导致RNN模型难以学习到<strong>远距离依赖</strong></li>
<li>过去的解决方法依赖于复杂的<strong>优化技术</strong>和<strong>网络架构</strong></li>
<li>提出一种较为<strong>简单</strong>的方式进行优化</li>
</ul>
<h2 id="过去的解决方法">过去的解决方法</h2>
<ul>
<li>用<strong>Hessian-Free</strong>来代替<strong>SGD</strong><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>（Hessian-Free可以关注到曲率）</li>
</ul>
<blockquote>
<p>虽然效果上有改进，但并不常用，原因可能如下</p>]]></description>
</item>
</channel>
</rss>
